# ðŸ“š Publications 
\* indicates equal contribution

## Preprint
---
[L-Eval: Instituting Standardized Evaluation for Long Context Language Models](https://arxiv.org/pdf/2307.11088.pdf) \| [L-Eval ![](https://img.shields.io/github/stars/OpenLMLab/LEval?style=social)](https://github.com/OpenLMLab/LEval)

Chenxin An, **Shansan Gong**, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, Xipeng Qiu

---

[In-Context Learning with Many Demonstration Examples](https://arxiv.org/pdf/2302.04931.pdf) \| [EVALM ![](https://img.shields.io/github/stars/Shark-NLP/EVALM?style=social)](https://github.com/Shark-NLP/EVALM)

Mukai Li, **Shansan Gong**, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, Lingpeng Kong

---
## Published

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2023 Findings</div><img src='images/emnlp23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffuSeq-v2: Bridging Discrete and Continuous Text Spaces for Accelerated Seq2Seq Diffusion Models](https://arxiv.org/pdf/2310.05793.pdf)

**Shansan Gong**, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong

[Code](https://github.com/Shark-NLP/DiffuSeq/tree/diffuseq-v2)\| Accelerated version of DiffuSeq, where the discrete noise bridges the training and sampling stages, saving time consumption of these two stages.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/iclr23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[DiffuSeq: Sequence to Sequence Text Generation With Diffusion Models](https://arxiv.org/pdf/2210.08933.pdf)

**Shansan Gong**, Mukai Li, Jiangtao Feng, Zhiyong Wu, Lingpeng Kong

[DiffuSeq ![](https://img.shields.io/github/stars/Shark-NLP/DiffuSeq?style=social)](https://github.com/Shark-NLP/DiffuSeq) <strong><span class='show_paper_citations' data='F86VNoMAAAAJ:2osOgNQ5qMEC'></span></strong>  | [Poster](./uploads/DiffuSeq_poster-v1.pdf) \|
<!-- - Our proposed **DiffuSeq** is trained end-to-end in a classifier-free manner, targeting Seq2Seq tasks. -->
<!-- - We establish a theoretical connection among AR, NAR and DiffuSeq models (refer to our original paper). -->
DiffuSeq is a powerful model for text generation, matching or even surpassing competitive AR, iterative NAR, and PLMs on quality and diversity.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACL 2023 Industry</div><img src='images/acl23.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Transferable and Efficient: Unifying Dynamic Multi-Domain Product Categorization](https://aclanthology.org/2023.acl-industry.46/) 

**Shansan Gong**\*, Zelin Zhou\*, Shuo Wang, Fengjiao Chen, Xiujie Song, Xuezhi Cao, Yunsen Xian, Kenny Zhu

[Data](https://github.com/ze-lin/TaLR) \| [Poster](./uploads/TaLR-poster.pdf) \| A new framework to unify the categorization process as well as leverage knowledge from different domains.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">SIGIR 2022</div><img src='images/sigir22.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation](https://dl.acm.org/doi/10.1145/3477495.3532040)

**Shansan Gong**, Kenny Q. Zhu

[TCAR ![](https://img.shields.io/github/stars/summmeer/session-based-news-recommendation?style=social)](https://github.com/summmeer/session-based-news-recommendation) | [Slides](./uploads/SIGIR22-fp1153-slides.pdf)\|
By leveraging different kinds of implicit feedback, we alleviate the trade-off between the precision and diversity.
<!-- - We tackle user/article cold-start problem to some extend, which is effective for real-world application. -->

</div>
</div>