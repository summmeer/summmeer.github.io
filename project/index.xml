<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects | Sansa's Site</title><link>https://summmeer.github.io/project/</link><atom:link href="https://summmeer.github.io/project/index.xml" rel="self" type="application/rss+xml"/><description>Projects</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 May 2022 00:00:00 +0000</lastBuildDate><image><url>https://summmeer.github.io/media/icon_hu4465e7aec766cd42246149649a25868d_28750_512x512_fill_lanczos_center_3.png</url><title>Projects</title><link>https://summmeer.github.io/project/</link></image><item><title>Dynamic Product Categorization for Multiple Domain</title><link>https://summmeer.github.io/project/productcate/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/productcate/</guid><description>&lt;p>Product categorization is to assign a product with a suitable category, which is usually organized in a predefined taxonomy. As e-commerce platforms develop different business lines, a special but challenging categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time.&lt;/p>
&lt;p>In order to unify the categorization process and jointly utilize the cross-domain data, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space.&lt;/p>
&lt;p>However, pure vector matching may fall for the surface form of text, and we thus further leverage the universal &amp;ldquo;knowledge&amp;rdquo; across different business domains to complement textual semantics. We design a heuristic retrieval strategy and pretrain a contrastive ranking model with the help of &amp;ldquo;concept&amp;rdquo;, which resembles the shared keyword knowledge cross domains.&lt;/p>
&lt;p>Our comprehensive experiments show that our method outperforms existing sophisticated approaches quantitatively and efficiently on dynamical multi-domain taxonomies.&lt;/p></description></item><item><title>Postweb Query Intention Detection Model</title><link>https://summmeer.github.io/project/queryintent/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/queryintent/</guid><description>&lt;p>In order to optimize the search result, it is necessary to judge whether the intents of different search Query1 and Query2 are consistent, which is formulated as a binary classification task. Based on the cross-language pre-training model InfoXLM, I train the downstream binary classification task, that is, cross-language Query intent detection, and introduce PostWeb information (ie, text information such as the title, link, and abstract of the Query search result) as an additional feature of the Query. The classification model increases the AUC by about 1% on the test data set while achieving around 5% improvement on the financial stock test data set.&lt;/p>
&lt;p>First I mined the training/test data from Bing Search Log and Scraper, to fetch the &lt;code>title&lt;/code>, &lt;code>URL&lt;/code> and &lt;code>snippet&lt;/code> as additional features. Then I built the training pipeline, and train the post-web QQ model. Then, I evaluate the performance of the post-web model on different languages, which shows the improvement is more apparent in minor languages such as ja, ko, zh. When fixing the precision of the model at 98%, the number of recalled items (using post-web model) is more than 10% of the pre-web model.&lt;/p>
&lt;p>Second, I did some work on the model attention layer visualization, using two python package &lt;code>bertviz&lt;/code> and &lt;code>lit&lt;/code>, then did the case study. By analysis of bad cases (the bad case is a case that the pre-web model predict correctly while the post web not), I have some observations:&lt;/p>
&lt;ul>
&lt;li>sometimes the model cannot catch the attention relation between similar words;&lt;/li>
&lt;li>too many snippet words make the model confused;&lt;/li>
&lt;li>the post web information (search result) is not accurate;&lt;/li>
&lt;li>need to improve the quality of labeling.&lt;/li>
&lt;/ul></description></item><item><title>Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation</title><link>https://summmeer.github.io/project/newsrec/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/newsrec/</guid><description>&lt;p>In this paper, we are interested in exploiting user actions outside the clicks themselves. We call them &amp;ldquo;implicit feedback&amp;rdquo;. Typical implicit feedback can be extracted from browsing the main page, reading an article, closing an article, backtracking, etc. We believe that modeling such implicit feedback &amp;ldquo;explicitly&amp;rdquo; in the session-based recommendation system can help the recommender understand user intention better. In this work, we focus on answering these questions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If a user clicked an article, did she really like it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If a user did not click an article, did she dislike it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How do we model the temporal characteristics of the user and the articles in the system?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>First, in traditional recommendation systems, “clicks” usually indicate a “like” or a vote from the user, but things are a bit different for news reading. Users may be “tricked” into clicking an article and once they realize that, they will quickly back out and switch to other articles. Thus the time a user spends on reading an article, than just the click alone, which is only binary. We model this as the implicit positive feedback in this paper.&lt;/p>
&lt;p>Second, just because the user did not click on an article does not necessarily mean the user does not like it; maybe she was never exposed to this article! We can infer what articles might have an impression on the user during a session by assuming that articles are presented to the user roughly in the order of their publication time. Only those articles within her list of impressions but not clicked are considered &amp;ldquo;not interesting&amp;rdquo; to her. This is called implicit negative feedback.&lt;/p>
&lt;p>Finally, while the positive and negative feedback helps us estimate the connection between the user and the articles, some critical temporal information is useful to model the user and the articles individually. The session start time of a user may suggest the daily routine of that user. We can expect users who read on the same day of one week or the same time of one day to have to share the same reading behavior or even background. On the other hand, the publishing time of each article can also be formed into a sequence in a session, which reflects the user’s sensitivity to the timeliness of the stories. We thus carefully design the representation of session start time and article publishing time as implicit neutral feedback.&lt;/p></description></item><item><title>Iwenbooks Development</title><link>https://summmeer.github.io/project/iwen/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/iwen/</guid><description>&lt;p>Iwen is a book and news reading APP, which currently provides service for more than 20,000 users. Many useful modules are implemented in this APP which help users to better understand the reading materials, for example, the TTS(text-to-speech), auto-translation, auto-question-generation and so on.&lt;/p>
&lt;p>Besides the role of CTO and team management. I mainly responsible for the back-end development, the database maintenance and the implementation of recommendation algorithm.&lt;/p>
&lt;ul>
&lt;li>Design the database structure for the news/user data and develop API for the front-end.&lt;/li>
&lt;li>Implement the vanilla book/news recommendation system, which is based on a hybrid strategy combining collaborative filtering and user portraits. For books, I consider content information like the abstract and timeliness of books in the users&amp;rsquo; history to model users&amp;rsquo; interests. For news, I summarize the topic that each user may be interested in and adjust the recommendation score personalizely for them in real time.&lt;/li>
&lt;/ul></description></item><item><title>A Search Engine from scratch based on the WikiPedia data</title><link>https://summmeer.github.io/project/wing/</link><pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/wing/</guid><description>&lt;p>Wikipedia is a well-known free online encyclopedia, created and edited by volunteers around the world, and it offers free copies of all available content, where we can get the latest complete dump of the English-language Wikipedia. Thus, we build a Wikipedia-based search engine: Wing, with 5 sort algorithms, a well-designed GUI, and the responsive time scale of millisecond. Wing is short for &amp;lsquo;&amp;lsquo;Wing Is Not Google&amp;rsquo;&amp;rsquo;.&lt;/p>
&lt;p>After we download the 17GB zip file of the whole raw data and unzip it, we use a Wiki-Extractor tool to extract around 10 million documents. We use SPIMI (Single-Pass In-Memory Indexing) algorithm to construct a posting list.&lt;/p>
&lt;h3 id="algorithm-optimize">Algorithm Optimize&lt;/h3>
&lt;p>TF-IDF based methods will give short articles high weights. So, there will be a problem that some docs whose content is very short but not very relative to the query, have high weights in ranking results. Thus, when we calculate the term frequency, we increase the term frequency of the words that appear in the title. Through this way, we can filter some short but meaningless docs and get what we want.&lt;/p>
&lt;p>We optimized TF-IDF with PageRank score, which is computed to measure the authority of pages.&lt;/p>
&lt;h3 id="developemnt">Developemnt&lt;/h3>
&lt;p>We adopt Flask to enable our back end. Flask is a microframework for web applications written in Python. The front end is implemented with Vue. Vue is a progressive framework for building user interfaces. Also, we use Element UI, a desktop component library, to make our interface more beautiful and user-friendly. The front end includes three pages: welcome page, query page, and article page. It is well known that Python program is running as a single process and is very slow. To speed up our program written by Python, we use &lt;code>Multiprocess&lt;/code> package to accelerate our for-loop.&lt;/p></description></item></channel></rss>