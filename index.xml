<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Sansa's Site</title><link>https://summmeer.github.io/</link><atom:link href="https://summmeer.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Sansa's Site</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 01 Jul 2022 00:00:00 +0000</lastBuildDate><image><url>https://summmeer.github.io/media/icon_hu4465e7aec766cd42246149649a25868d_28750_512x512_fill_lanczos_center_3.png</url><title>Sansa's Site</title><link>https://summmeer.github.io/</link></image><item><title>Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation</title><link>https://summmeer.github.io/publication/newsrec/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/publication/newsrec/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Survey list of diffusion models</title><link>https://summmeer.github.io/post/diffusion-notes/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/post/diffusion-notes/</guid><description>&lt;h3 id="classical-model-unconditional">Classical model (unconditional)&lt;/h3>
&lt;h4 id="1-diffusion-original">1. Diffusion original&lt;/h4>
&lt;ul>
&lt;li>Jascha Sohl-Dickstein et al. &lt;em>Deep Unsupervised Learning using Nonequilibrium Thermodynamics.&lt;/em> ICML 2015.&lt;/li>
&lt;/ul>
&lt;h4 id="2-ncsn-and-ncsnv2-noise-conditioned-score-network">2. NCSN and NCSNv2 (noise-conditioned score network)&lt;/h4>
&lt;p>Score-based generative modeling + Langevin dynamics&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Yang Song &amp;amp; Stefano Ermon. &lt;em>Generative modeling by estimating gradients of the data distribution.&lt;/em> NeurIPS 2019.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Yang Song &amp;amp; Stefano Ermon. &lt;em>Improved techniques for training score-based generative models.&lt;/em> NeurIPS 2020. (for higher resolution)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="3-ddpm">3. DDPM&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Jonathan Ho et al. &lt;em>Denoising diffusion probabilistic models.&lt;/em> NeurlPS 2020.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Alex Nichol &amp;amp; Prafulla Dhariwal. &lt;em>Improved denoising diffusion probabilistic models ICML 2021.&lt;/em> (ImageNet)&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="4-ddim-non--markov-chain-to-speed-up">4. DDIM non- Markov chain to speed up&lt;/h4>
&lt;ul>
&lt;li>Jiaming Song et al. &lt;em>Denoising diffusion implicit models.&lt;/em> ICLR 2021.&lt;/li>
&lt;/ul>
&lt;h3 id="domain">Domain&lt;/h3>
&lt;h4 id="1-image">1. Image&lt;/h4>
&lt;ul>
&lt;li>Prafula Dhariwal &amp;amp; Alex Nichol. &lt;em>Diffusion Models Beat GANs on Image Synthesis.&lt;/em> NeurIPS 2021.&lt;/li>
&lt;/ul>
&lt;h4 id="2-time-series">2. Time series&lt;/h4>
&lt;ul>
&lt;li>Yang Song, &lt;em>CSDI: Conditional Score-based Diffusion Models for Probabilistic Time Series Imputation&lt;/em>, NeurIPS 2021&lt;/li>
&lt;/ul>
&lt;h4 id="3-text">3. Text&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Nikolay Savinov, Aaron van den Oord. &lt;em>Step-unrolled Denoising Autoencoders for Text Generation.&lt;/em> ICLR 2022. [SUNDAE &lt;a href="https://github.com/vvvm23/sundae" target="_blank" rel="noopener">code non-official&lt;/a>] (baseline: Disco, CMLM)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Emiel Hoogeboom Max Welling. &lt;em>Argmax flows and multinomial diffusion: Towards non-autoregressive language models.&lt;/em> NeurIPS 2021. [&lt;a href="https://github.com/ehoogeboom/multinomial_diffusion" target="_blank" rel="noopener">code&lt;/a>] (image+text)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Jacob Austin, Rianne van den Berg. &lt;em>Structured denoising diffusion models in discrete state-spaces.&lt;/em> NeurIPS 2021. (image+text)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>AUTOREGRESSIVE DIFFUSION MODELS.&lt;/em> ICLR 2022 [&lt;a href="https://github.com/heejkoo/Awesome-Diffusion-Models" target="_blank" rel="noopener">code&lt;/a>]&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Lisa Xiang, &lt;em>Diffusion-LM Improves Controllable Text Generation&lt;/em>, NeurIPS 2022. [&lt;a href="https://github.com/XiangLi1999/Diffusion-LM" target="_blank" rel="noopener">code&lt;/a>]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="4-speech">4. Speech&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Gautam Mittal, Ian Simon. &lt;em>Symbolic music generation with diffusion models.&lt;/em> ISMIR 2021. (SMG)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>FastDiff: A Fast Conditional Diffusion Model for High-Quality Speech Synthesis&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>WaveGrad: Estimating Gradients for Waveform Generation.&lt;/em> ICLR 2021&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h4 id="5-text-to-image">5. Text-to-Image&lt;/h4>
&lt;ul>
&lt;li>
&lt;p>Chitwan Saharia. &lt;em>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Aditya Ramesh, Mark Chen. &lt;em>Hierarchical Text-Conditional Image Generation with CLIP Latents.&lt;/em>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;em>KNN-Diffusion: Image Generation via Large-Scale Retrieval&lt;/em>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="compare-text-generation">Compare Text Generation&lt;/h3>
&lt;h4 id="1-ar-vs-nar">1. AR vs NAR:&lt;/h4>
&lt;ul>
&lt;li>ARDM uses order-agnostic AR (parallelized)
NAR: multinomial diffusion, D3PM, SUNDAE, SMG, Diffusion-LM&lt;/li>
&lt;/ul>
&lt;h4 id="2-continuous-vs-discrete">2. Continuous vs discrete:&lt;/h4>
&lt;ul>
&lt;li>ARDM, D3PM, SUNDAE is discrete (clear transition)
Diffusion-LM is continuous, SMG first uses VAE to encode&lt;/li>
&lt;/ul>
&lt;h4 id="3-controllable">3. Controllable:&lt;/h4>
&lt;ul>
&lt;li>Diffusion-LM uses plug and play&lt;/li>
&lt;li>WaveGrad uses condition with diffusion model&lt;/li>
&lt;li>CSDI: condition on input sequence&lt;/li>
&lt;/ul></description></item><item><title>Dynamic Product Categorization for Multiple Domain</title><link>https://summmeer.github.io/project/productcate/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/productcate/</guid><description>&lt;p>Product categorization is to assign a product with a suitable category, which is usually organized in a predefined taxonomy. As e-commerce platforms develop different business lines, a special but challenging categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time.&lt;/p>
&lt;p>In order to unify the categorization process and jointly utilize the cross-domain data, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space.&lt;/p>
&lt;p>However, pure vector matching may fall for the surface form of text, and we thus further leverage the universal &amp;ldquo;knowledge&amp;rdquo; across different business domains to complement textual semantics. We design a heuristic retrieval strategy and pretrain a contrastive ranking model with the help of &amp;ldquo;concept&amp;rdquo;, which resembles the shared keyword knowledge cross domains.&lt;/p>
&lt;p>Our comprehensive experiments show that our method outperforms existing sophisticated approaches quantitatively and efficiently on dynamical multi-domain taxonomies.&lt;/p></description></item><item><title>Postweb Query Intention Detection Model</title><link>https://summmeer.github.io/project/queryintent/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/queryintent/</guid><description>&lt;p>In order to optimize the search result, it is necessary to judge whether the intents of different search Query1 and Query2 are consistent, which is formulated as a binary classification task. Based on the cross-language pre-training model InfoXLM, I train the downstream binary classification task, that is, cross-language Query intent detection, and introduce PostWeb information (ie, text information such as the title, link, and abstract of the Query search result) as an additional feature of the Query. The classification model increases the AUC by about 1% on the test data set while achieving around 5% improvement on the financial stock test data set.&lt;/p>
&lt;p>First I mined the training/test data from Bing Search Log and Scraper, to fetch the &lt;code>title&lt;/code>, &lt;code>URL&lt;/code> and &lt;code>snippet&lt;/code> as additional features. Then I built the training pipeline, and train the post-web QQ model. Then, I evaluate the performance of the post-web model on different languages, which shows the improvement is more apparent in minor languages such as ja, ko, zh. When fixing the precision of the model at 98%, the number of recalled items (using post-web model) is more than 10% of the pre-web model.&lt;/p>
&lt;p>Second, I did some work on the model attention layer visualization, using two python package &lt;code>bertviz&lt;/code> and &lt;code>lit&lt;/code>, then did the case study. By analysis of bad cases (the bad case is a case that the pre-web model predict correctly while the post web not), I have some observations:&lt;/p>
&lt;ul>
&lt;li>sometimes the model cannot catch the attention relation between similar words;&lt;/li>
&lt;li>too many snippet words make the model confused;&lt;/li>
&lt;li>the post web information (search result) is not accurate;&lt;/li>
&lt;li>need to improve the quality of labeling.&lt;/li>
&lt;/ul></description></item><item><title>Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation</title><link>https://summmeer.github.io/project/newsrec/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/newsrec/</guid><description>&lt;p>In this paper, we are interested in exploiting user actions outside the clicks themselves. We call them &amp;ldquo;implicit feedback&amp;rdquo;. Typical implicit feedback can be extracted from browsing the main page, reading an article, closing an article, backtracking, etc. We believe that modeling such implicit feedback &amp;ldquo;explicitly&amp;rdquo; in the session-based recommendation system can help the recommender understand user intention better. In this work, we focus on answering these questions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If a user clicked an article, did she really like it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If a user did not click an article, did she dislike it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How do we model the temporal characteristics of the user and the articles in the system?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>First, in traditional recommendation systems, “clicks” usually indicate a “like” or a vote from the user, but things are a bit different for news reading. Users may be “tricked” into clicking an article and once they realize that, they will quickly back out and switch to other articles. Thus the time a user spends on reading an article, than just the click alone, which is only binary. We model this as the implicit positive feedback in this paper.&lt;/p>
&lt;p>Second, just because the user did not click on an article does not necessarily mean the user does not like it; maybe she was never exposed to this article! We can infer what articles might have an impression on the user during a session by assuming that articles are presented to the user roughly in the order of their publication time. Only those articles within her list of impressions but not clicked are considered &amp;ldquo;not interesting&amp;rdquo; to her. This is called implicit negative feedback.&lt;/p>
&lt;p>Finally, while the positive and negative feedback helps us estimate the connection between the user and the articles, some critical temporal information is useful to model the user and the articles individually. The session start time of a user may suggest the daily routine of that user. We can expect users who read on the same day of one week or the same time of one day to have to share the same reading behavior or even background. On the other hand, the publishing time of each article can also be formed into a sequence in a session, which reflects the user’s sensitivity to the timeliness of the stories. We thus carefully design the representation of session start time and article publishing time as implicit neutral feedback.&lt;/p></description></item><item><title>Iwenbooks Development</title><link>https://summmeer.github.io/project/iwen/</link><pubDate>Thu, 30 Sep 2021 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/iwen/</guid><description>&lt;p>Iwen is a book and news reading APP, which currently provides service for more than 20,000 users. Many useful modules are implemented in this APP which help users to better understand the reading materials, for example, the TTS(text-to-speech), auto-translation, auto-question-generation and so on.&lt;/p>
&lt;p>Besides the role of CTO and team management. I mainly responsible for the back-end development, the database maintenance and the implementation of recommendation algorithm.&lt;/p>
&lt;ul>
&lt;li>Design the database structure for the news/user data and develop API for the front-end.&lt;/li>
&lt;li>Implement the vanilla book/news recommendation system, which is based on a hybrid strategy combining collaborative filtering and user portraits. For books, I consider content information like the abstract and timeliness of books in the users&amp;rsquo; history to model users&amp;rsquo; interests. For news, I summarize the topic that each user may be interested in and adjust the recommendation score personalizely for them in real time.&lt;/li>
&lt;/ul></description></item><item><title>A Search Engine from scratch based on the WikiPedia data</title><link>https://summmeer.github.io/project/wing/</link><pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/wing/</guid><description>&lt;p>Wikipedia is a well-known free online encyclopedia, created and edited by volunteers around the world, and it offers free copies of all available content, where we can get the latest complete dump of the English-language Wikipedia. Thus, we build a Wikipedia-based search engine: Wing, with 5 sort algorithms, a well-designed GUI, and the responsive time scale of millisecond. Wing is short for &amp;lsquo;&amp;lsquo;Wing Is Not Google&amp;rsquo;&amp;rsquo;.&lt;/p>
&lt;p>After we download the 17GB zip file of the whole raw data and unzip it, we use a Wiki-Extractor tool to extract around 10 million documents. We use SPIMI (Single-Pass In-Memory Indexing) algorithm to construct a posting list.&lt;/p>
&lt;h3 id="algorithm-optimize">Algorithm Optimize&lt;/h3>
&lt;p>TF-IDF based methods will give short articles high weights. So, there will be a problem that some docs whose content is very short but not very relative to the query, have high weights in ranking results. Thus, when we calculate the term frequency, we increase the term frequency of the words that appear in the title. Through this way, we can filter some short but meaningless docs and get what we want.&lt;/p>
&lt;p>We optimized TF-IDF with PageRank score, which is computed to measure the authority of pages.&lt;/p>
&lt;h3 id="developemnt">Developemnt&lt;/h3>
&lt;p>We adopt Flask to enable our back end. Flask is a microframework for web applications written in Python. The front end is implemented with Vue. Vue is a progressive framework for building user interfaces. Also, we use Element UI, a desktop component library, to make our interface more beautiful and user-friendly. The front end includes three pages: welcome page, query page, and article page. It is well known that Python program is running as a single process and is very slow. To speed up our program written by Python, we use &lt;code>Multiprocess&lt;/code> package to accelerate our for-loop.&lt;/p></description></item><item><title/><link>https://summmeer.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/admin/config.yml</guid><description/></item></channel></rss>