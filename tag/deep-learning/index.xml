<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning | Sansa's Site</title><link>https://summmeer.github.io/tag/deep-learning/</link><atom:link href="https://summmeer.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml"/><description>Deep Learning</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 May 2022 00:00:00 +0000</lastBuildDate><image><url>https://summmeer.github.io/media/icon_hu4465e7aec766cd42246149649a25868d_28750_512x512_fill_lanczos_center_3.png</url><title>Deep Learning</title><link>https://summmeer.github.io/tag/deep-learning/</link></image><item><title>Dynamic Product Categorization for Multiple Domain</title><link>https://summmeer.github.io/project/productcate/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/productcate/</guid><description>&lt;p>Product categorization is to assign a product with a suitable category, which is usually organized in a predefined taxonomy. As e-commerce platforms develop different business lines, a special but challenging categorization scenario emerges, where there are multiple domain-specific category taxonomies and each of them evolves dynamically over time.&lt;/p>
&lt;p>In order to unify the categorization process and jointly utilize the cross-domain data, we propose a two-stage taxonomy-agnostic framework that relies solely on calculating the semantic relatedness between product titles and category names in the vector space.&lt;/p>
&lt;p>However, pure vector matching may fall for the surface form of text, and we thus further leverage the universal &amp;ldquo;knowledge&amp;rdquo; across different business domains to complement textual semantics. We design a heuristic retrieval strategy and pretrain a contrastive ranking model with the help of &amp;ldquo;concept&amp;rdquo;, which resembles the shared keyword knowledge cross domains.&lt;/p>
&lt;p>Our comprehensive experiments show that our method outperforms existing sophisticated approaches quantitatively and efficiently on dynamical multi-domain taxonomies.&lt;/p></description></item><item><title>Postweb Query Intention Detection Model</title><link>https://summmeer.github.io/project/queryintent/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/queryintent/</guid><description>&lt;p>In order to optimize the search result, it is necessary to judge whether the intents of different search Query1 and Query2 are consistent, which is formulated as a binary classification task. Based on the cross-language pre-training model InfoXLM, I train the downstream binary classification task, that is, cross-language Query intent detection, and introduce PostWeb information (ie, text information such as the title, link, and abstract of the Query search result) as an additional feature of the Query. The classification model increases the AUC by about 1% on the test data set while achieving around 5% improvement on the financial stock test data set.&lt;/p>
&lt;p>First I mined the training/test data from Bing Search Log and Scraper, to fetch the &lt;code>title&lt;/code>, &lt;code>URL&lt;/code> and &lt;code>snippet&lt;/code> as additional features. Then I built the training pipeline, and train the post-web QQ model. Then, I evaluate the performance of the post-web model on different languages, which shows the improvement is more apparent in minor languages such as ja, ko, zh. When fixing the precision of the model at 98%, the number of recalled items (using post-web model) is more than 10% of the pre-web model.&lt;/p>
&lt;p>Second, I did some work on the model attention layer visualization, using two python package &lt;code>bertviz&lt;/code> and &lt;code>lit&lt;/code>, then did the case study. By analysis of bad cases (the bad case is a case that the pre-web model predict correctly while the post web not), I have some observations:&lt;/p>
&lt;ul>
&lt;li>sometimes the model cannot catch the attention relation between similar words;&lt;/li>
&lt;li>too many snippet words make the model confused;&lt;/li>
&lt;li>the post web information (search result) is not accurate;&lt;/li>
&lt;li>need to improve the quality of labeling.&lt;/li>
&lt;/ul></description></item><item><title>Positive, Negative and Neutral: Modeling Implicit Feedback in Session-based News Recommendation</title><link>https://summmeer.github.io/project/newsrec/</link><pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/newsrec/</guid><description>&lt;p>In this paper, we are interested in exploiting user actions outside the clicks themselves. We call them &amp;ldquo;implicit feedback&amp;rdquo;. Typical implicit feedback can be extracted from browsing the main page, reading an article, closing an article, backtracking, etc. We believe that modeling such implicit feedback &amp;ldquo;explicitly&amp;rdquo; in the session-based recommendation system can help the recommender understand user intention better. In this work, we focus on answering these questions:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If a user clicked an article, did she really like it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If a user did not click an article, did she dislike it?&lt;/p>
&lt;/li>
&lt;li>
&lt;p>How do we model the temporal characteristics of the user and the articles in the system?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>First, in traditional recommendation systems, “clicks” usually indicate a “like” or a vote from the user, but things are a bit different for news reading. Users may be “tricked” into clicking an article and once they realize that, they will quickly back out and switch to other articles. Thus the time a user spends on reading an article, than just the click alone, which is only binary. We model this as the implicit positive feedback in this paper.&lt;/p>
&lt;p>Second, just because the user did not click on an article does not necessarily mean the user does not like it; maybe she was never exposed to this article! We can infer what articles might have an impression on the user during a session by assuming that articles are presented to the user roughly in the order of their publication time. Only those articles within her list of impressions but not clicked are considered &amp;ldquo;not interesting&amp;rdquo; to her. This is called implicit negative feedback.&lt;/p>
&lt;p>Finally, while the positive and negative feedback helps us estimate the connection between the user and the articles, some critical temporal information is useful to model the user and the articles individually. The session start time of a user may suggest the daily routine of that user. We can expect users who read on the same day of one week or the same time of one day to have to share the same reading behavior or even background. On the other hand, the publishing time of each article can also be formed into a sequence in a session, which reflects the user’s sensitivity to the timeliness of the stories. We thus carefully design the representation of session start time and article publishing time as implicit neutral feedback.&lt;/p></description></item></channel></rss>