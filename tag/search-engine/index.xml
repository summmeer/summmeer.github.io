<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Search Engine | Sansa's Site</title><link>https://summmeer.github.io/tag/search-engine/</link><atom:link href="https://summmeer.github.io/tag/search-engine/index.xml" rel="self" type="application/rss+xml"/><description>Search Engine</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 30 May 2022 00:00:00 +0000</lastBuildDate><image><url>https://summmeer.github.io/media/icon_hu4465e7aec766cd42246149649a25868d_28750_512x512_fill_lanczos_center_3.png</url><title>Search Engine</title><link>https://summmeer.github.io/tag/search-engine/</link></image><item><title>Postweb Query Intention Detection Model</title><link>https://summmeer.github.io/project/queryintent/</link><pubDate>Mon, 30 May 2022 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/queryintent/</guid><description>&lt;p>In order to optimize the search result, it is necessary to judge whether the intents of different search Query1 and Query2 are consistent, which is formulated as a binary classification task. Based on the cross-language pre-training model InfoXLM, I train the downstream binary classification task, that is, cross-language Query intent detection, and introduce PostWeb information (ie, text information such as the title, link, and abstract of the Query search result) as an additional feature of the Query. The classification model increases the AUC by about 1% on the test data set while achieving around 5% improvement on the financial stock test data set.&lt;/p>
&lt;p>First I mined the training/test data from Bing Search Log and Scraper, to fetch the &lt;code>title&lt;/code>, &lt;code>URL&lt;/code> and &lt;code>snippet&lt;/code> as additional features. Then I built the training pipeline, and train the post-web QQ model. Then, I evaluate the performance of the post-web model on different languages, which shows the improvement is more apparent in minor languages such as ja, ko, zh. When fixing the precision of the model at 98%, the number of recalled items (using post-web model) is more than 10% of the pre-web model.&lt;/p>
&lt;p>Second, I did some work on the model attention layer visualization, using two python package &lt;code>bertviz&lt;/code> and &lt;code>lit&lt;/code>, then did the case study. By analysis of bad cases (the bad case is a case that the pre-web model predict correctly while the post web not), I have some observations:&lt;/p>
&lt;ul>
&lt;li>sometimes the model cannot catch the attention relation between similar words;&lt;/li>
&lt;li>too many snippet words make the model confused;&lt;/li>
&lt;li>the post web information (search result) is not accurate;&lt;/li>
&lt;li>need to improve the quality of labeling.&lt;/li>
&lt;/ul></description></item><item><title>A Search Engine from scratch based on the WikiPedia data</title><link>https://summmeer.github.io/project/wing/</link><pubDate>Sun, 30 May 2021 00:00:00 +0000</pubDate><guid>https://summmeer.github.io/project/wing/</guid><description>&lt;p>Wikipedia is a well-known free online encyclopedia, created and edited by volunteers around the world, and it offers free copies of all available content, where we can get the latest complete dump of the English-language Wikipedia. Thus, we build a Wikipedia-based search engine: Wing, with 5 sort algorithms, a well-designed GUI, and the responsive time scale of millisecond. Wing is short for &amp;lsquo;&amp;lsquo;Wing Is Not Google&amp;rsquo;&amp;rsquo;.&lt;/p>
&lt;p>After we download the 17GB zip file of the whole raw data and unzip it, we use a Wiki-Extractor tool to extract around 10 million documents. We use SPIMI (Single-Pass In-Memory Indexing) algorithm to construct a posting list.&lt;/p>
&lt;h3 id="algorithm-optimize">Algorithm Optimize&lt;/h3>
&lt;p>TF-IDF based methods will give short articles high weights. So, there will be a problem that some docs whose content is very short but not very relative to the query, have high weights in ranking results. Thus, when we calculate the term frequency, we increase the term frequency of the words that appear in the title. Through this way, we can filter some short but meaningless docs and get what we want.&lt;/p>
&lt;p>We optimized TF-IDF with PageRank score, which is computed to measure the authority of pages.&lt;/p>
&lt;h3 id="developemnt">Developemnt&lt;/h3>
&lt;p>We adopt Flask to enable our back end. Flask is a microframework for web applications written in Python. The front end is implemented with Vue. Vue is a progressive framework for building user interfaces. Also, we use Element UI, a desktop component library, to make our interface more beautiful and user-friendly. The front end includes three pages: welcome page, query page, and article page. It is well known that Python program is running as a single process and is very slow. To speed up our program written by Python, we use &lt;code>Multiprocess&lt;/code> package to accelerate our for-loop.&lt;/p></description></item></channel></rss>